\documentclass[a4paper,oneside]{memoir}

\input{thesis/preamble}

%%  Begin document
%%  ==================================================================
\begin{document}

\input{thesis/titlepage}

\chapter*{Abstract}

\input{thesis/abstract}

\newpage

\tableofcontents*

\chapter{Introduction}
High level, memory managed languages offer many productivity advantages because
developers don't have to worry about how objects are arranged in memory. However,
memory layout can hugely affect performance so the productivity advantages can
come with efficiency disadvantages. Much research has been done not only to
quantify this discrepancy but also to narrow the gap.

In this thesis I will be looking at the memory layout of semi-space garbage
collector in the context of a purely functional programming language.
I will implement two optimizations in the garbage collector and evaluate these
optimizations against the baseline. These two novel techniques, as well as the
detailed measurements of their effects, are the primary contributions of this
report.

\section{Jikes RVM vs. native}
\makecomment{Why I use a native GC rather than the Jikes RVM (Research Virtual Machine)
to test my hypothesis even though a lot of other people use it.
Answer: Haskell has different allocation patterns than Java*}

\section{Research Aims}
I have implemented a new semi-space garbage collector for a subset of the
Haskell programming language. This collector contains two novel optimizations
that modify how objects are arranged in memory. It is my hypothesis that these
two optimizations will improve how the garbage collector interacts with the
memory caches. As part of this work I want to answer the following questions:
\begin{itemize}
  \item Is the implementation unreasonably complicated? Some previously proposed
    systems have shown promise on a small scale but were too complicated to be
    used outside of academia.\cite{GHCImmix}
  \item Are the optimizations beneficial and do the benefits outweigh the costs?
  \item Can the optimizations be ported to collectors used in industry?
\end{itemize}

\section{Thesis organization}
Chapter 2 goes over the current approaches to garbage collection, how people
have tried to solve the problem in the past, and provides details about
LHC which are required by the later chapters.
Chapter 3 describes the design and implementation of the main contribution of
this thesis, as well as the design and implementation of a further optimization
that follows as a direct consequence of the tail-copy technique.
Chapter 4 evaluates these two new approaches against a baseline collector. This
evaluation looks at deterministic factors which are proxies for performance.
Chapter 5 concludes the work and presents possible future work.

\chapter{Background}
\section{Processors, memory and cache}

\begin{figure}
  \centering
  \includesvg[width=0.5\linewidth,inkscapelatex=false]{images/D7.svg}
  \caption{Memory hierarchy.}
\end{figure}

CPUs used to run at similar frequencies as main memory, allowing them to read
from memory at roughly the same speed that it could process the information.
Sometimes programs has to process larger data sets than could fit into memory
and parts of the data set would have to be swapped out of main memory and onto
a disk or tape. A lot of caching research from that time was focused on how to
manage the virtual memory using limited physical memory (which supports direct
access with a constant latency) and a spinning disk drive (which requires seeking).

In recent years CPUs have gotten a lot faster, the bandwidth of memory
hasn't been able to keep pace and the memory latency has even gotten worse. This
means that fetching data from memory can easily be the bottleneck when processing
a data set. To solve this, several levels of memory caches were inserted closer
to the CPU. These caches work in levels with the lowest levels being smallest
and fastest. The caches have latencies that are roughly 100x to 10x quicker
than main memory. The caches also have higher bandwidth than main memory but
the benefit is less extreme here.

The introduction of caches meant that memory fetches no longer completed in a
constant amount of time because of the difference between a cache hit and
a cache miss. Just as seen with spinning disks earlier, certain access patterns
and data layouts can significantly improve performance. The canonical problems
that highlights this is matrix multiplication: The elements of the resulting
matrix can be computed in any order but choosing a cache-friendly access pattern
can be orders of magnitude faster.

\begin{table}
  \centering
  \caption{Approximate access times for memory in an Intel Pentium M}
  \begin{tabular}{c | c}
    To Where & Cycles \\ \hline
    Register & $\le 1$ \\
    L1d & $\sim 3$ \\
    L2 & $\sim 14$ \\
    Main Memory & $\sim 240$
  \end{tabular}
\end{table}

% Write about MESI and cache associativity

\section{Manual memory management}
Still widely used in low-level languages and on embedded systems.
The onus is on the user to ensure correct usage.
Modern languages are usually garbage collected (8/10 most popular languages on
github\cite{GithubLanguages} in 2017 were garbage collected).
Manual management has low overhead and can finalize objects immediately when
they're no longer used. This is important for limited resources such as file
handlers, file locks, etc.
Manual management often has more predictable performance without arbitrary pauses
but performs less well in terms of throughput and locality.

\section{Reference counting}
\section{Mark-and-sweep}
\begin{figure}[b]
  \centering
  \includesvg[width=0.8\linewidth,inkscapelatex=false]{images/D9.svg}
  \caption{Mark-and-Sweep.}
\end{figure}

Unlike manual management and reference counting which never scans through all
the heap allocated objects, tracing garbage collection involves finding all live
heap objects by following a root set of objects out through all their children
and children's children. Once all the live objects have been found, unused memory
and resources can be reclaimed.
The mark-and-sweep does exactly this in two phases: First all objects reachable
from the root set are marked, then the "sweep" phases reclaims unused memory and
releases unused OS resources.
This algorithm correctly reclaims cyclic data structures when they're no longer
referenced but, similarly to manual management and reference counting, leads to
fragmentation and thus poor cache performance.

\section{Copying collectors}
\begin{figure}
  \centering
  \includesvg[height=2cm,inkscapelatex=false]{images/D8.svg}
  \caption{Cheney's semi-space algorithm.}
\end{figure}
\begin{figure}[b]
  \centering
  \includesvg[height=2cm,inkscapelatex=false]{images/D10.svg}
  \caption{Moon's algorithm.}
\end{figure}

In 1969, Fenichel et al published an algorithm for copying the reachable objects
into newly allocated memory chunk rather than merely marking them.\cite{Fenichel} Once the copying
is complete, all previously used memory will no longer contain live objects
and can be reclaimed. This makes the "sweep" phases unnecessary and it gets rid
of any fragmentation at the cost of requiring 2x available memory. The published
algorithm used stack space to traverse and copy the heap objects which is
undesirable since the stack space is limited and garbage collection will therefore
fail if the heap grows too large. One year later, Cheney published a non-recursive
algorithm for copying live objects and this algorithm has become the foundation
for essentially all modern copying garbage collectors. Since the algorithm is
non-recursive, it uses no stack space and works on heaps at any size.

Cheney's and Fenichel's algorithms both solve the same problem of copying a graph
with Cheney's algorithm having the decisive advantage of working on graphs of
unlimited size. However, the two algorithms also differ in the fact that Fenichel
copied the graph in depth-first order and Cheney copies using breadth-first. This
different copy order affects cache performance but the effects wasn't too pronounced
with the architecture at the time and it wasn't unless 1984 that Moon et al
investigated the effect copy order has with virtual memory. And it wasn't until
1995 that the effects of copy order was quantified in relationship to the
CPU memory cache hierarchy by Nakashima and Chikayama.

Moon modified Cheney's algorithm to have two scanning pointers instead of one.
The to-space is divided up into blocks and the second scanning pointer always
points at the same block as the free pointer. Since objects are read (scavenged)
from the scanning pointer and children are copied (evacuated) to the free pointer,
this means the algorithm often only touches a single block even when the graph
is large and the free pointer is many blocks away from the first scanning pointer.
Since this is optimizing for block/page access, it can perform better when memory
is limited and page faults are expensive.

Moon's algorithm was later improved by Wilson, Lam, and Moher to reduce the
number of objects that are scavenged twice (scavenging is idempotent). Siegwart
and Hirzel further improved performance by adapting the algorithm for
multi-processor machines. Both groups of authors focused primarily on reducing
cache and TLB misses.

\section{Generational collectors}
\makecomment{Diagrams for this section: generational gc.}

It was found empirically that most allocated objects become unreachable (die)
almost immediately and that very few objects live for a significant fraction
of a programs total run-time. This effect of high infant mortality is exploited
by generational collectors by splitting the heap into two or more separate areas
(aka generations) that can be collected independently. A common configuration is
to have a minor and a major generation: New objects are allocated into the minor
generation and promoted into the major generation if they survive a collection.
Since it's relatively rare for older objects to contain pointers to younger
objects, only a small amount of bookkeeping is necessary to collect the minor
generation without tracing through all the live objects in the major generation.
In other words, infant objects can be allocated and collected without tracing or
copying long lived objects. This approach has proved so successful that, even
though the exact rate of infant mortality and object longevity varies from
program to program, generational GC is the default in most popular garbage
collected languages.

\section{Region inference}


\section{Haskell}
\makecomment{Haskell is a purely functional language.\cite{HaskellReport} Mutable objects are
uncommon which leads to a high allocate rate (ie. in Haskell, you create new
objects instead of mutating old ones. How many Haskellers does it take to
change a broken lightbulb? Zero. The lightbulb is immutable. You just create
a new house with a working lightbulb.)}

\section{GHC}
\makecomment{Diagrams for this section: GHC's memory model, GHC's pipeline.}

\makecomment{GHC's object model is assumes an open-world and is extendible.
Objects as pointers has drawbacks, though, and GHC has abandoned the tag-less
model. However, they can only use 2-3 bits (on 32bit and 64bit machines,
respectively) so their tagging is limited. LHC assumes a closed world and therefore
sacrifices extendibility for more tagging bits. LHC can use nearly a full word
for tagging.}

\section{LHC}
\makecomment{Which parts of LHC's architecture is important to describe? \cite{LHC}
Include: Simplicity of the RTS, Compiles subset of Haskell, Generates high-level
LLVM code, Similar memory model to GHC, Whole-program compiler.}

\makecomment{Explain indirections. They are important for the implementation
details of tail-pointer elimination.}

\section{Related work}
\subsection{Depth-first copying (two approaches)}
\subsection{immix}
\subsection{custom object layout (java)}

\chapter{Tail-copying and tail-compaction}

\section{Concept overview}
\label{concept_overview}

\makecomment{Diagrams for this section: Copy order and cache lines (depth-first,
breadth-first, tail-first), tail pointer elimination.}

While it has been shown that tracing garbage collectors that use a depth-first
copying algorithm have superior data locality, this technique is not used in any
of the ten most popular languages on GitHub. The overheads in turns of memory
usage and runtime performance are simply too great and the benefits only
manifests themselves in niche situations (eg. with limited physical memory and
a high cost of page faults). However, I think a compromise is possible that
maintains most of the data locality benefits without hampering performance in
the common case, making such an algorithm suitable as the default garbage
collector rather than just being useful in special cases.

Between depth-first copying and breadth-first copying, there exist a middle-ground
which I call tail-first copying. In this scheme, exactly one child of each node
(the tail) is copied depth-first and the rest of the children (if there are any)
are copied breadth-first. This has several potential benefits.
\begin{itemize}
  \item
  Firstly, as one
  child is likely (but not guaranteed to be) next to its parent, the mutator could
  experience improved locality if it traverses the spine of a data structure (ie.
  down the length of a list or down the right-most path of a tree).
  \item
  Secondly, and
  more importantly, the tail-first copying algorithm operates over blocks of
  tail-linked nodes and is less likely to interleave unrelated nodes. In contrast,
  depth-first copying of two linked lists results in each node of the lists being
  interleaved. And if the heap was extended to include a third list, that list
  would be interleaved with the first two. This means memory is typically not
  copied sequentially which leads to more costly cache misses.
  \item
  The final benefit
  is that tail-first copying does not require any reserved book-keeping data in
  each node nor does it require any extra stack space. The copying algorithm that
  places children next to their parents is tail-recursive (ie it uses a fixed
  amount of stack space), and, just like the breadth-first algorithm, a single
  pointer can be used to keep track of which objects have been scavenged and which
  have not.
\end{itemize}

Tail-first copying in inherently optimistic as there are many situations that
make it impossible to put a child node right after its parent. Each of these
situations have to be detected and they add complexity as well as logic branches
to the algorithm. There are three key cases to consider:
\begin{enumerate}
  \item Multiple parents. Objects may have any number of parents but can only be
  placed next to one. In this case, the child would be placed next to the first
  parent and all subsequently traversed parents will be placed potentially far
  away in memory. I don't believe this to be a big problem as, while having
  multiple is definitely not uncommon, the majority of objects should only have
  a single parent.
  \item
  Unlucky root set traversal. If object A is the parent of object B and the root
  set contains both objects, there's a chance that object B will be copied
  (evacuated) before object A and it therefore cannot be placed immediately
  after object A. This should be a minor problem as root set traversals happen
  infrequently and the size of the root sets are fixed at compile-time.
  \item
  Generational barriers. In generational garbage collectors, objects are
  separated by age into different areas (called generations). Objects in
  separate generations must live apart and cannot be placed right next to each
  other. Fortunately, as objects in older generations rarely point to objects in
  younger generations and due to techniques such as eager promotion (moving an
  object to an older generation if it has parents in that generation), data
  structures are rarely interspersed over many generations. Instead, chunks of
  linked data tends to be located in the same generation, with possible links
  to chunks of data in other generations. As such, I don't believe generational
  garbage collecting would significantly affect the behavior of tail-copying.
  That said, this report will focus exclusively on adding tail-copying to a
  non-generational garbage collector.
\end{enumerate}

For nodes that cannot be placed immediately next to their parent, tail-first
copying is identical to breadth-first copying.
\newline
\newline
After the live objects in a heap have been traced by the tail-first copying
algorithm, many of the objects will have a peculiar property in common: They'll
contain a pointer to a child located right after it. That is, these objects
of size $S$ and located at position $X$, will contain a pointer to a child at
location $X+S$. This common, repeated pattern can be compressed to a single bit
set in the parent's object header: If the bit is set, the last child pointer is
omitted from the object itself and instead calculated as $X+S$.

If nothing else is changed, this compression of the heap would lead to a smaller
allocation area (since the allocation area is sized as a multiple of the live heap
size)
and the garbage collector would therefore be invoked more frequently. On the other
hand, it is also possible to use the extra heap space to extend the allocation area
which would make the garbage collector run less frequently. Both options have
their uses, but, for the sake of comparing apples to apples, I've chosen the
middle ground of keeping the allocation area size fixed across the three garbage
collection strategies. This should make the heaps slightly smaller but keep the
number of garbage collections the same.

Switching between breadth-first, depth-first, and tail-first copy order does not
change the semantic layout of the heap objects and this can therefore be done
without changing any other parts of the evaluator or the runtime system. In
contrast, omitting the tail pointer \emph{does} change the memory model and
all systems that interact with the heap objects will need to take it into
account.



\section{Implementation}

\subsection{Baseline garbage collector}\label{algorithm}

Before tail-first copy order and tail-pointer elimination can be implemented,
a semi-space garbage collector needs to be written for LHC. While LHC didn't
ship with a semi-space garbage collector when this project began, it does have
excellent support for finding the roots of the heap which happens to be one of
the most difficult parts of a garbage collector.

The garbage collector that I've implemented in LHC is nearly identical to the
algorithm put forth in Cheney's "A Non-recursive List Compacting Algorithm"
without any of the improvements that were later invented (such as cache-sized
nurseries, stepping, or generations). The algorithm has five steps and is run
when new allocations have hit a certain limit:
\begin{enumerate}
  \item Allocate a \emph{to-space} of the same size as the previous heap. This
  ensures that there will be enough space to potentially copy the entire heap.
  \item Marking: Evacuate (copy object without copying children) everything in the root-set.
  \item Scavenging: For each object in \emph{to-space}, evacuate its children. Once all
  the objects have been scavenged, the entire heap will have been traced and all
  live objects are copied.
  \item Extend the \emph{to-space} with a new allocation area. This allocation
  area is the size of the \emph{to-space} multiplied by a factor of 3. That is,
  if \emph{to-space} contains, say, 2 megabytes of objects, the new allocation
  area will allow $2*3=6$ megabytes of new objects to be allocated before another
  round of garbage collection is triggered. For performance reasons, the allocation
  area cannot be smaller than 1 megabyte.
  \item Free the old heap and declare \emph{to-space} to be the new heap.
\end{enumerate}

LHC's garbage collection API made it simple to implement the above algorithm
without having to worry how the rest of the compiler is constructed.
Additionally, the garbage collection API is timed with high-precision clocks
to give accurate summary of how much time is spent in the mutator compared to
the garbage collector.

\subsection{Tail-first copy order}

\makecomment{Diagram: Flow-chart of the new 'evacuate' procedure.}

The copy order of the garbage collector is entirely determined by the 'evacuate'
procedure in the algorithm outlined in \ref{algorithm}. This produce takes the
address of a heap object and performs the following steps:
\begin{enumerate}
  \item If the object is an indirection, follow the indirection. This step is
  repeated until an actual heap object is found.
  \item Check if the object is already in the \emph{to-space}. If so, nothing
  more needs to be done and we can exit the procedure.
  \item Lookup the size of the object and copy it to the next free spot in
  \emph{to-space}.
  \item Update the given address of the heap object to point to its new position
  in the \emph{to-space}.
  \item Overwrite the old heap object with an indirection to its new position.
\end{enumerate}

In some Haskell compilers, like GHC, indirections take up two words of memory
(one for the tag and one for the pointer). This can cause problems when you're
trying to overwrite heap objects that are smaller than two words. Luckily, in
LHC, indirections only take up a single word and no special precautions are
necessary. LHC is able to squeeze an indirection into a single word because
all heap objects are word aligned and pointers to them therefore don't use the
least significant 2 or 3 bits (for 32bit and 64bit systems, respectively).

A key thing to note is that the 'evacuate' procedure only copies a single heap
object and not the children of that object. This was the key changed proposed
by Cheney to Fenichel's algorithm which recursively copied all children as well.
\cite{Fenichel,Cheney:1970} Between these two approaches, there's a middle
ground: Recursively copy a single child (if it exists). Doing so would be more
likely to put related objects next to each other than breadth-first copy, and
it still wouldn't require the stack space that a depth-first copy does.

Introducing a check for children to the procedure, and recursing if it is true,
does not add a significant amount of complexity. However, it is important to
verify that the procedure is indeed tail-recursive. Otherwise it'll use an
unbounded amount of stack space and undoubtedly fail for sufficiently large
heaps.


\subsection{Tail-pointer elimination}

\begin{figure}[t]
  \includesvg[height=3.5cm, inkscapelatex=false]{images/D6.svg}
  \caption{Evacuation of both child and parent nodes.}
  \label{parent_child}
\end{figure}

\begin{figure}[b]
  \includesvg[height=3.5cm, inkscapelatex=false]{images/D3.svg}
  \caption{Evacuation of compacted parent and child.}
  \label{compacted_parent_child}
\end{figure}

\begin{figure}[t]
  \includesvg[height=3.5cm, inkscapelatex=false]{images/D2.svg}
  \caption{Evacuation of compacted parent and modified child.}
  \label{modified_child}
\end{figure}

The elimination of tail pointers is straight forward in the common case but there
are two situations that add complexity: Firstly, objects are always mutated by
replacing them with an indirection to a new object. In contrast to inplace updates,
this approach allows small objects to be replaced by larger objects and the update
happens atomically. The indirections are removed during garbage collection
and should not inhibit tail-pointer elimination as long as all other requirements
are met.
Secondly, objects may have multiple parents but only a single pointer to an object
can be eliminated. The pointer elimination happens on a first-come, first-served
basis and the order may change between garbage collections such that children get
placed next to a different parent each time. When this happens, the pointer
that was previously eliminated will have to be restored.

The tail-pointer elimination algorithm depends on whether the parent has a
tail-pointer or not and whether the child has already been evacuated or not. This
leads to four new cases that needs to be handled in the 'evacuate' procedure:
\begin{enumerate}
  \item Parent and child both in \emph{from-space}. Figure \ref{parent_child}
    illustrates
    how a compacted parent node is created in \emph{to-space} when both the parent
    and child nodes were in \emph{from-space}. It should be noted that this
    algorithm is recursive and the child node may itself be compacted if it
    itself is a parent and its child satisfies the requirements. The figure also
    shows that both the parent and child nodes in \emph{from-space} are replaced
    by indirections. This guarantees that nodes are never duplicated (ie. copied
    twice) during garbage collection, even when the nodes have multiple parents.

  \item Compacted parent and child both in \emph{from-space}. Figure
    \ref{compacted_parent_child} illustrates how compaction is maintained when
    the child has been evacuated or moved. If the child is still available, it
    is immediately moved together with its parent and no intermediate pointers
    have to be introduced. As the algorithm is recursive and the child may also
    be compacted, this leads to sequential blocks of memory being copied rather
    than individual objects that may be scattered in memory.

    Indirections that still point to \emph{from-space} can occur before any
    child node and that case needs to be handled as well. Figure
    \ref{modified_child} displays a situation where a child node was previously
    placed next to its parent but then later modified (ie. replaced with an
    indirection to a new object). Conceptually, indirections are transparent
    and do not have any effect on semantics but they do add complexity to the
    implementation. As a side note, a compacted parent with an indirection is
    isomorphic to an ordinary parent with a tail-pointer.
  \item Parent with evacuated child. Figure \ref{multiple_parents} illustrates
    what happens when a child node has already been evacuated. This can happen
    when the child has multiple parents or if the child was part of the initial
    root set. In this case, the parent cannot be placed right above the child
    and its therefore copied verbatim instead, keeping the tail-pointer intact.
  \item Compacted parent with evacuated child. Figure \ref{parent_no_child}
    illustrates a situation where the child node most likely has multiple parents
    and is now placed next to a different parent. Here a new tail-pointer has to
    be introduced between the parent and the child.
\end{enumerate}

\begin{figure}[b]
  \includesvg[height=3.5cm, inkscapelatex=false]{images/D4.svg}
  \caption{Evacuation of parent only.}
  \label{multiple_parents}
\end{figure}


\begin{figure}
  \includesvg[height=3.5cm, inkscapelatex=false]{images/D1.svg}
  \caption{Evacuation of compacted parent while introducing new child pointer.}
  \label{parent_no_child}
\end{figure}

Since the new 'evacuate' procedure guarantees that tail pointers always point
to \emph{to-space}, the scavenging procedure can be optimized to omit the
call to 'evacuate' on the tail pointer. The full code of the scavening procedure
is shown in appendix \ref{scavenge_code}.

To complete the implementation of tail-pointer elimination, the rest of the
runtime-system has to be aware of how to find the address of a child by either
following a pointer or computing it from the address of the parent. The LHC
compiler tries to be agnostic towards the layout of objects and provides
an API access point for finding the address of an object's last child. The full
code is in appendix \ref{loadLast_code} and it simply either follows the pointer
to the child of it exists or directly returns the address of the child if the
pointer has been eliminated.



\chapter{Evaluation}

\section{Methodology}
The effects of tail-copying and tail-compaction are measured over 7 small
benchmark programs with metrics collected in three different ways.
GC statistics (heap size, allocations, bytes copied) are reported by the LHC RTS,
and, since these metrics do not change between runs, the benchmark suite is only
executed once to collect this data.
How the runtime is split between the mutator and the garbage collector is
measured using a high-precision clock with a resolution of 1ns. These runtime
measurements are inherently stochastic and the benchmark suite is executed
fives and only the median values are used.
Hardware performance counter statistics are collected using the Linux \emph{perf}
tool.\cite{perf} This tool is used to collect the number of page faults, cycles, instructions,
branches, branch misses, LLC (last-level cache) loads and LLC load misses. Again,
these measurements are stochastic and the benchmark suite is executed five times
and the median values are used.

The \emph{perf} tool supports other counters but this subset was chosen since
each of these counters should something relevant to the performance of the two
novel algorithms. Page faults (in this benchmark suite) are triggered when newly
allocated memory is used for the first time. They find an unused page of physical
memory and make sure its filled with zeros. This can be quite time consuming
and much research has gone into reducing the number of page faults.\cite{Nakashima:1995,Chase:1987,Moon:1984,Grunwald:1993}
The number of cycles is related to the number of instructions but seemingly
innocent algorithmic changes can have unintuitive effects. With the right
scheduling, multiple instructions can be executed in parallel and latency from
the memory systems can be hidden (eg. by prefetching data). Getting this scheduling
wrong will make the cycle count go up even when the number of executed instructions
stays the same. Branches and particularly branch-misses are another factor in
how efficiently a CPU can execute instructions. A branch mispredict flushes the
instruction pipeline, which are quite deep in modern architectures, wasting
10-20 cycles.\cite{Drepper}

All measurements were gathered on an Intel i7-8705 3.1GHz running Linux 4.18 64bit.

Finally, for the reasons stated in section \ref{concept_overview}, tail-compaction
depends on tail-copying and automatically enables it.

\section{Verification}

\begin{table}
  \hspace*{-1cm}\begin{minipage}{0.5\linewidth}
    \centering
    \caption{"BinTree" pointer distances}
    \label{bintree_dist}
    \resizebox{!}{2cm}{\begin{tabular}{|l|c|c|}
      \hline
      \multicolumn{3}{|c|}{Object distances:}\\
      \hline
      \textbf{Bucket}&\textbf{Baseline}&\textbf{Tail-copy}\\
      <64 b        &523           &114053463\\
      <4 kb        &26349         &6616\\
      <64 kb       &423445        &211347\\
      <512 kb      &2810463       &1564876\\
      <2 mb        &7874711       &4414888\\
      >2 mb        &216971852     &107856153\\
      \hline
    \end{tabular}}
  \end{minipage}%
  \hspace*{2cm}\begin{minipage}{0.5\linewidth}
    \centering
    \caption{"Allocate" pointer distances}
    \label{allocate_dist}
    \resizebox{!}{2cm}{\begin{tabular}{|l|c|c|}
      \hline
      \multicolumn{3}{|c|}{Object distances:}\\
      \hline
      \textbf{Bucket}&\textbf{Baseline}&\textbf{Tail-copy}\\
      <64 b        &79930413     &79930435       \\
      <4 kb        &3378         &3360           \\
      <64 kb       &51200        &51200          \\
      <512 kb      &361736       &361736         \\
      <2 mb        &1029416      &1029408        \\
      >2 mb        &78484699     &78484703       \\
      \hline
    \end{tabular}}
  \end{minipage}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Heap size and allocations tables

\begin{table}[b]
  \begin{ltable}{Heap size}{heaptable}
    \data{
    \hline
\multicolumn{4}{|c|}{Heap size:}\\
\hline
\textbf{Name}&\textbf{Baseline}&\textbf{Tail-copy}&\textbf{Tail-compact}\\
Allocate       &1.8 gb         &+0.0\%         &-8.3\%         \\
BinTree        &1.8 gb         &+0.0\%         &-6.2\%         \\
Bush           &1.3 gb         &+0.0\%         &-3.1\%         \\
BushTail       &5.1 gb         &+0.0\%         &-1.7\%         \\
Imbalanced     &1.6 gb         &+0.0\%         &-6.2\%         \\
MemBench       &6.7 gb         &+0.0\%         &-5.0\%         \\
PowerSet       &10.4 gb        &+0.0\%         &-5.0\%         \\

\hline
\hline
\multicolumn{2}{|l|}{Min}&0.0\%&0.0\%\\
\multicolumn{2}{|l|}{Max}&0.0\%&0.0\%\\
\multicolumn{2}{|l|}{Mean}&0.0\%&0.0\%\\
\hline
    }
  \end{ltable}
  \begin{rtable}{Allocations}{alloctable}
    \data{
    \hline
\multicolumn{4}{|c|}{Allocated:}\\
\hline
\textbf{Name}&\textbf{Baseline}&\textbf{Tail-copy}&\textbf{Tail-compact}\\
Allocate       &4.0 gb         &+0.0\%         &+0.0\%         \\
BinTree        &9.8 gb         &+0.0\%         &+0.0\%         \\
Bush           &6.6 gb         &+0.0\%         &+0.0\%         \\
BushTail       &5.2 gb         &+0.0\%         &+0.0\%         \\
Imbalanced     &7.8 gb         &+0.0\%         &+0.0\%         \\
MemBench       &9.7 gb         &+0.0\%         &+0.0\%         \\
PowerSet       &15.1 gb        &+0.0\%         &+0.0\%         \\
\hline
\hline
\multicolumn{2}{|l|}{Min}&0.0\%&0.0\%\\
\multicolumn{2}{|l|}{Max}&0.0\%&0.0\%\\
\multicolumn{2}{|l|}{Mean}&0.0\%&0.0\%\\
\hline
    }
  \end{rtable}
\end{table}


The seven benchmark program used all have heap sizes measured in gigabytes,
making it nigh impossible to visually inspect for verification. Fortunately
there's a lot of aggregate data which can give us an indication of whether
tail-copying and tail-compaction works as theorized in section \ref{concept_overview}.

Table \ref{bintree_dist} and \ref{allocate_dist} show the pointer distances
between parent and child objects, grouped into 6 buckets. Siegwart et al uses
this metric of pointer distances to quantify the improvements of their
garbage collection algorithm.\cite{Siegwart:2006} Since the purpose of the tail-copying algorithm
is to put children right next to their parent, the number of pointers that refer
to objects within 64 bytes should increase dramatically and this is exactly what
is seen for the \emph{BinTree} benchmark. The \emph{Allocate} benchmark, however,
does not exhibit this pattern. This is due to the way the program manipulates
a single list: For lists, there's no significant difference between breadth-first
copying, depth-first copying or tail-first copying. The \emph{Allocate} benchmark
is an outlier, though, and none of the other programs behave like this (as
shown by the pointer distances in appendix \ref{pointer_distances}).

Furthermore, as shown by table \ref{alloctable}, neither of the two new strategies
have an effect on the number of allocations. This is exactly as expected since
both copying and compaction happens well \emph{after} objects are allocated.
Looking at table \ref{copiedtable}, it can be seen that the number of bytes copied
during garbage collection when tail-compaction is enabled, always drops by a
fraction: -\nicefrac{1}{3} or -33.3\%, -\nicefrac{1}{4} or -25\%,
-\nicefrac{1}{5} or -20\%, -\nicefrac{1}{8} or -12.5\%. This is because each
benchmark problem mainly uses a single data structure of fixed size. If the size
of this data structure is, say, 4 words, eliminating the one-word pointer would
yield a saving of \nicefrac{1}{4}. Finally, table \ref{heaptable} shows a modest
reduction in heap size when tail-compaction is enabled, just as predicted earlier.
These results indicate that the garbage collection strategies work as theorized.

\section{Performance interpretation}

\input{thesis/data}

Tail-copying does not change the code of the mutator but changing the layout of
the data may very well still affect the performance. Table \ref{muttable} shows
that both tail-copying and tail-compaction tends to improve the performance
of the mutator ever so slightly. This effect is so tiny as to be insignificant,
though, and the key insight to glean is that neither strategy hurts the performance
of the mutator.

GC times, as seen in table \ref{gctable}, reveils a more complicated landscape:
for both tail-copying and tail-compaction, three benchmarks become faster and
four benchmarks become slower. It is important to note, however, that all seven
benchmarks are improved by tail-compaction compared to just tail-copying.
Understanding the performance of tail-copying and tail-compaction requires a
deeper dig into the performance counters.

Table \ref{pagetable} shows that enable tail-compaction reduces page faults
across the board and the drop corresponds almost exactly to the reduced heap
size. This, together with LLC loads which are also reduced across the board,
appears to be why tail-compaction improves the performance of every single
benchmark program (relative to tail-copying) even though the total number of
branches go up (table \ref{branchtable}).

The reason \emph{Allocate}, \emph{BinTree}, \emph{Bush} and \emph{BushTail}
perform so poorly appears to be due to CPU stalls: Number of cycles go up
while number of instructions go down (as shown in tables \ref{cycletable} and
\ref{insttable}). In the cases of \emph{BinTree}, \emph{Bush} and
\emph{BushTail}, this appears to be primarily caused by a large increase in
branch misses, ranging from +44.4\% to +107.5\% (see table \ref{branchmisstable}),
and partly due to a small increase in LLC load misses in the case of \emph{Bush}
(see table \ref{llcmisstable}). \emph{Allocate} follows a different pattern
with a reduction in both branch misses and LLC loads but a sizable increase in
LLC load misses, leading to a poor instructions-per-cycle count (see table
\ref{llctable}).

Only four of the benchmark programs (\emph{BinTree}, \emph{Imbalanced},
\emph{MemBench} and \emph{PowerSet}) exhibit the reduction in LLC load misses
that tail-copying and tail-compaction aims for. Furthermore, the benefits of
the reduction in LLC load misses is impeded by the extra cost of branch misses
in the case of \emph{Imbalanced}. For \emph{BinTree} the situation is even worse
and the benefits are dwarfed by the extra costs. All in all, the two new
garbage collection strategies succeed in arranging the heap in a more
cache-frindly manner but the gains are eaten by unpredictable branches.

\section{Possible improvements}



\section{Code complexity}

The implementation of tail-copying takes up only 15 lines out of the 224 lines
that define the full semispace garbage collector in LHC. The implementation of
tail-compaction only takes up 10 lines in the RTS but leans heavily on the
garbage collection API in LHC which clocks in at 230 lines of Haskell code. All
in all, both the tail-copying and tail-compaction algorithms fit in well in
the semispace garbage collector and do not impose excessive complexity.

\chapter{Conclusion}


\chapter*{Support}

\makecomment{The following quotes show how the references will be used to support
the introduction, implementation decisions, and conclusion. This chapter will
not appear in the final version of this report.}

\blockquote{Recent process speed improvements have far outpaced memory speed
improvements.}\cite{Novark:2006}

\blockquote{Past work has shown that careful placement of objects in memory can
yield performance improvements as high as 42\% in C and C++ applications.}
\cite{Novark:2006}

\blockquote{In general, software references locality can be improved either by
changing a program's data access pattern or its data organization and layout.}
\cite{Chilimbi:1999}

\blockquote{The speed of microprocessors has increased 60\% per year for almost
two decades. Yet, over the same period, the time to access main memory only
decreased at 10\% per year.}
\cite{Chilimbi:1999}

\blockquote{Careful placement of structure elements provides the mechanism to
improve the cache locality of pointer manipulating programs and, consequently,
their performance.}
\cite{Chilimbi:1999}

\blockquote{As processor speeds continue to improve relative to main-memory
access times, cache performance is becoming an increasingly important component
of program performance.}
\cite{Reinhold:1994}

\blockquote{Processors are getting faster, by as factor of 1.5 to 2 per year,
while the speed of main-memory chips is improving only slowly.}
\cite{Reinhold:1994}

\blockquote{In this paper, we propose two copying garbage collection methods.
These two methods copy data structures in depth-first order, while conventional
method copies in breadth-first order. This modification greatly improves memory
access locality during both garbage collection and computation after.\cite{Nakashima:1995}}

\blockquote{In a computer with virtual memory both mark-and-sweep and
reference-counting garbage collection tend to exhibit poor locality of reference
and may run slowly because of excessive page faulting.}
\cite{Chase:1987}

\blockquote{The goal of hierarchical copying is to reduce cache and TLB misses
by colocating objects on the same cache line or page. [...] Paralle hierarchical
copying reduce mutator misses on all measured levels of the memory subsystem:
L1 data cache, combined L2 cache, and TLB.\cite{Siegwart:2006}}


\blockquote{Approximately depth-first copying improves the locality of
data structures copied by the garbage collector.}
\cite{Moon:1984}

\blockquote{A copying garbage collector is free to choose the order in which
it copies accessible objects. It can explit this freedom to improve locality by
copying related objects onto the same page.}
\cite{Moon:1984}

\blockquote{A copying garbage collector can choose breadth-first copying or
depth-first copying. The Cheney list-copying algorithm, which is traditionally
used because it does not require any temporary storage (such as a stack), is
breadth-first.\cite{Moon:1984}}


\blockquote{Depth-first copying generally yields better locality than
breadth-first copying, because it tends to put components of a structure on the
same page as the parent structure.}
\cite{Moon:1984}

\makecomment{Jikes RVM and MMtk are explained here.}\cite{Blackburn:2004}

\blockquote{To reduce fragmentation in mark-region and other non-moving
collectors, this paper introduces opportunistic defragmentation which mixes
copying and marking in a single pass. Using these building blocks, we introduce
immix, a novel high performance garbage collector that combines mark-region with
opportunistic defragmentation to achieve space efficiency, fast collection, and
continguous allocation for mutator performance.\cite{Blackburn:2008}}

\blockquote{This paper shows that, with enough memory on the computer, it is
more expensive to explicitly free a cell than it is to leave it for the garbage
collector - even if the cost of freeing a cell is only a single machine
instruction.}
\cite{Appel:1987}

\blockquote{The traditional mark-and-sweep algorithm, which puts all free cells
onto a linked list for later re-use, takes time proportional to the numebr of
reachable cells plus the number of garbage cells.}
\cite{Appel:1987}

\blockquote{Garbage-collected systems such as Smalltalk, Lisp, or actor languages typically violate some of the basic locality assumptions of modern memory hierarchies, leading to poor performance of normal caching strategies. The main problem is cyclic reuse of memory at a time scale too long to be captured by any caching policy. \cite{Lam:1992}}


\blockquote{Given our understanding of the memory reuse cycle of a generational
garbage collector, we believe it is attractive to exploit the genertional
property by using a cache large enough to hold the youngest generation.]}
\cite{Lam:1992}

\blockquote{In a garbage collected environment, multithreaded programs can run
into an "allocation wall", in which performance is limited by the rate at which
newly allocated data can be written to main memory, and adding more cores does
not improve performance once the limit is reached. One way to avoid the
allocation wall is to use a generational collector with per-thread nurseries
each smaller than the size of the L2 cache, so that most memory accesses hit the
cache rather than main memory. \cite{Marlow:2011}}

\blockquote{The effectiveness problem is that reference counting fails to reclaim
circular structures. If the pointers in a group of objects create a cycle, the
objects' reference counts are never reduced to zero, even if there is no path
to the objects from the root set. \cite{Wilson:1992}}


\blockquote{The efficiency problem with reference counting is that its cost is
generally proportional to the amount of work done by the running program, with a
fairly large constant of proportionality.}
\cite{Wilson:1992}

\blockquote{There are three major problems with traditinal mark-sweep garbage
collectors. First, it is difficult to handle objects of varying sizes without
fragmentation of the available memory. [...] The second problem with mark-sweep
collection is that the cost of a collection is proportional to the size of the
heap, including both live and garbage objects. [...] The third problem involves
locality of reference. Since objects are never moved, the live objects remain in
place after a collection, interspersed with free space. \cite{Wilson:1992}}

\blockquote{The graph of records and pointers is mostly acyclic: newer records
point to older records, and older records do not point to newer records.}
\cite{Appel:1989}

\blockquote{A young object is more likely to become garbage soon. Most objects
have short lifetimes, and only a few objects have long lifetimes. If something
has lasted for a long time already, then it’s probably part of a semi-permanent
data structure; if the object is relatively new, it’s likely to be a temporary
structure holding an intermediate result. \cite{Appel:1989}}

\blockquote{Matrix multiplication is a classical bench-mark for experimenting
with techniques used to exploit machine architecture and to overcome the
limitations of contemporary memory subsystems.}
\cite{Eiron:1999}

\blockquote{This research aims at advancing the state of the art of algorithm
engineering by balancing instruction level parallelism, two levels of data
tiling, copying to provably avoid any cache conficts, and prefetching in
parallel to algorithmic operations, in order to fully exploit the memory
bandwidth. Measurements show that the resultant matrix multiplication algorithm
outperforms IBM's ESSL by 6.8-31.8\%, is less sensitive to the size of the
input data, and scales better. \cite{Eiron:1999}}

\blockquote{As the gap between CPU and memory performance continues to grow, so
does the importance of e ective utilization of the memory hierarchy.}
\cite{Eiron:1999}

\blockquote{Temporal garbage collection (TGC) is based on two heuristics:
• Young objects tend lo become garbage quickly.
• The number of pointers from old objects to young objects is relatively small.}
\cite{Courts:1988}

\blockquote{Our measurements show that poor locality in sequential-fit
allocation algorithms reduces program performance, both by increasing paging
and cache miss rates. While increased paging can be debilitating on any
architecture, cache misses rates are also important for modern computer
architectures. \cite{Grunwald:1993}}

\blockquote{The algorithm avoids the need for recursion by using the partial
structure as it is built up to keep track of those lists that have been copied.}
\cite{Cheney:1970}

LLVM\cite{LLVM}

LLVM GC support\cite{LLVMGC}

GHC memory model \cite{GHCCommentary}

What every programmer should know about memory \cite{Drepper}

\bibliographystyle{unsrt}
\bibliography{sources}

\appendix
\lstset{language=c}

\chapter{Scavenging routine.}
\label{scavenge_code}

\begin{lstlisting}
static void scavenge(void) {
  const InfoTable *table;
  // make a local copy of the global 'scavenged' pointer for
  // better optimization opportunities.
  word* s = scavenged;
  // branch on _lhc_enable_tail_copying out side the loop to
  // avoid unnecessary branches later.
  if(_lhc_enable_tail_copying) {
    while(s < free_space) {
      word header = *s;
      table = &_lhc_info_tables[_lhc_getTag(header)];
      s += 1+table->nPrimitives;
      // i=1 because we want to skip the last pointer. The
      // pointer may not exist and if it does, it'll already
      // have been evacuated.
      for(int i=1;i<table->nHeapPointers;i++) {
        evacuate(NULL, 0, (word**)s);
        s++;
      }
      if(table->nHeapPointers && !_lhc_getTail(header)) s++;
    }
  } else {
    while(s < free_space) {
      table = &_lhc_info_tables[_lhc_getTag(*s)];
      s += 1+table->nPrimitives;
      for(int i=0;i<table->nHeapPointers;i++) {
        evacuate(NULL, 0, (word**)s);
        s++;
      }
    }
  }
  scavenged = s;
}
\end{lstlisting}

\chapter{loadLast routine.}
\label{loadLast_code}

\begin{lstlisting}
word* _lhc_loadLast(word *ptr, word idx) {
  if(_lhc_getTail(*ptr)) {
    return ptr+idx;
  } else {
    return ((word**)ptr)[idx];
  }
}
\end{lstlisting}

\chapter{Pointer distances}
\label{pointer_distances}

\begin{table}[t]
  \hspace*{-1cm}\begin{minipage}{0.5\linewidth}
    \centering
    \caption{"Bush" pointer distances}
    \resizebox{!}{2cm}{\begin{tabular}{|l|c|c|}
      \hline
      \multicolumn{3}{|c|}{Object distances:}\\
      \hline
      \textbf{Bucket}&\textbf{Baseline}&\textbf{Tail-copy}\\
      <64 b        &122           &41819119\\
      <4 kb        &13250         &4232\\
      <64 kb       &163606        &110942\\
      <512 kb      &1221204       &1067398\\
      <2 mb        &3568091       &3461401\\
      >2 mb        &162307510     &120810691\\
      \hline
    \end{tabular}}
  \end{minipage}%
  \hspace*{2cm}\begin{minipage}{0.5\linewidth}
    \centering
    \caption{"BushTail" pointer distances}
    \resizebox{!}{2cm}{\begin{tabular}{|l|c|c|}
      \hline
      \multicolumn{3}{|c|}{Object distances:}\\
      \hline
      \textbf{Bucket}&\textbf{Baseline}&\textbf{Tail-copy}\\
      <64 b        &63486839           &83491247           \\
      <4 kb        &4266         &865           \\
      <64 kb       &53303        &38177           \\
      <512 kb      &418026       &402746           \\
      <2 mb        &1231390       &1428856           \\
      >2 mb        &78308537     &58140470           \\
      \hline
    \end{tabular}}
  \end{minipage}
\end{table}%
\begin{table}
  \hspace*{-1cm}\begin{minipage}{0.5\linewidth}
    \centering
    \caption{"Imbalanced" pointer distances}
    \resizebox{!}{2cm}{\begin{tabular}{|l|c|c|}
      \hline
      \multicolumn{3}{|c|}{Object distances:}\\
      \hline
      \textbf{Bucket}&\textbf{Baseline}&\textbf{Tail-copy}\\
      <64 b        &440           &88991152\\
      <4 kb        &31107         &2089\\
      <64 kb       &507161        &72776\\
      <512 kb      &3428454       &721718\\
      <2 mb        &8952659       &2434901\\
      >2 mb        &165061074     &85758259\\
      \hline
    \end{tabular}}
  \end{minipage}%
  \hspace*{2cm}\begin{minipage}{0.5\linewidth}
    \centering
    \caption{"MemBench" pointer distances}
    \resizebox{!}{2cm}{\begin{tabular}{|l|c|c|}
      \hline
      \multicolumn{3}{|c|}{Object distances:}\\
      \hline
      \textbf{Bucket}&\textbf{Baseline}&\textbf{Tail-copy}\\
      <64 b        &29959     &132133061           \\
      <4 kb        &2860192         &71           \\
      <64 kb       &56830312        &13479           \\
      <512 kb      &204545590       &83117           \\
      <2 mb        &0      &177361           \\
      >2 mb        &0     &131858964           \\
      \hline
    \end{tabular}}
  \end{minipage}
\end{table}%
\begin{table}
    \centering
    \caption{"PowerSet" pointer distances}
    \resizebox{!}{2cm}{\begin{tabular}{|l|c|c|}
      \hline
      \multicolumn{3}{|c|}{Object distances:}\\
      \hline
      \textbf{Bucket}&\textbf{Baseline}&\textbf{Tail-copy}\\
      <64 b        &500068           &204577727\\
      <4 kb        &4133952         &0\\
      <64 kb       &99486789        &0\\
      <512 kb      &305010962       &47458\\
      <2 mb        &8       &190440\\
      >2 mb        &23602     &204339756\\
      \hline
    \end{tabular}}
\end{table}

% \chapter{Tikz}
% Tikz trial:
%
% \begin{tikzpicture}
%   \draw[step=.5cm,gray,very thin] (-1.4,-1.4) grid (1.4,1.4);
%   \draw (-1.5,0) -- (1.5,0);
%   \draw (0,-1.5) -- (0,1.5);
%   \draw (0,0) circle [radius=1cm];
%   \fill[green!20!white, draw=green!50!black] (0,0) -- (3mm,0mm)
%     arc [start angle=0, end angle=30, radius=3mm] -- cycle;
% \end{tikzpicture}


\end{document}
%%  ==================================================================
%%  End document
