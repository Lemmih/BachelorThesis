\documentclass[a4paper,oneside]{memoir}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{wallpaper}
\usepackage{palatino}
\usepackage{url}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{multirow}
\usepackage[autostyle]{csquotes}

\newcommand{\makecomment}[1]{{\color{red} Comment: #1}}


\newenvironment{ltable}[2]
{\hspace*{-2.5cm}\begin{minipage}{0.5\linewidth}
  \centering
  \caption{#1}
  \label{#2}}
{\end{minipage}}

\newenvironment{rtable}[2]
{\hspace*{3cm}\begin{minipage}{0.5\linewidth}
  \centering
  \caption{#1}
  \label{#2}}
{\end{minipage}}

\newcommand{\data}[1]{{
\resizebox{8cm}{!}{%
\begin{tabular}{|l|c|c|c|}
  #1
\end{tabular}}
}}

\setcounter{secnumdepth}{3}
% \setcounter{tocdepth}{3}

\input{styles/jenor}
\input{styles/hansen}

% \usepackage{titlesec, blindtext, color}
% \definecolor{gray75}{gray}{0.75}
% \newcommand{\hsp}{\hspace{20pt}}
% \titleformat{\chapter}[hang]{\Huge\bfseries}{\thechapter\hsp\textcolor{gray75}{|}\hsp}{0pt}{\Huge\bfseries}

% Setup captions
%\captionstyle[\centering]{\centering}
%\changecaptionwidth
%\captionwidth{0.8\linewidth}

% Protect against widows and orphans
%\clubpenalty=10000
%\widowpenalty=10000

%\linespread{1.2}

%\raggedbottom

% \chapterstyle{ger}
% \chapterstyle{ell}
% \chapterstyle{companion}
% \chapterstyle{hangnum}
% \chapterstyle{lyhne} % OKish
\chapterstyle{madsen} % OK
% \chapterstyle{jenor} % Font too big
% \chapterstyle{wilsondob}
% \chapterstyle{hansen} % OK

%\maxsecnumdepth{subsection}

%%  Setup fancy style quotation
%%  ==================================================================
%\usepackage{tikz}
%\usepackage{framed}

%\newcommand*\quotefont{\fontfamily{fxl}} % selects Libertine for quote font

% Make commands for the quotes
%\newcommand*{\openquote}{\tikz[remember picture,overlay,xshift=-15pt,yshift=-10pt]
%     \node (OQ) {\quotefont\fontsize{60}{60}\selectfont``};\kern0pt}
%\newcommand*{\closequote}{\tikz[remember picture,overlay,xshift=15pt,yshift=5pt]
%     \node (CQ) {\quotefont\fontsize{60}{60}\selectfont''};}

% select a colour for the shading
%\definecolor{shadecolor}{rgb}{1,1,1}

% wrap everything in its own environment
%\newenvironment{shadequote}%
%{\begin{snugshade}\begin{quote}\openquote}
%{\hfill\closequote\end{quote}\end{snugshade}}

%%  Begin document
%%  ==================================================================
\begin{document}

%%  Begin title page
%%  ==================================================================
    \thispagestyle{empty}
    \ULCornerWallPaper{1}{cover/nat-farve.pdf}
    \ULCornerWallPaper{1}{cover/nat-en.pdf}
    \begin{adjustwidth}{-3cm}{-1.5cm}
    \vspace*{-1cm}
    \textbf{\Huge Bachelor Thesis} \\
    \vspace*{2.5cm} \\
    \textbf{\Huge Cache optimizations in garbage collection} \\
    \vspace*{.1cm} \\
    {\huge Opportunistically selecting cache-frindly object layouts} \\
    \begin{tabbing}
    % adjust the hspace below for the longest author name
    David Himmelstrup \hspace{1cm} \= \texttt{<vrs552@alumni.ku.dk>} \\
    \\[12cm]
    \textbf{\Large Supervisor} \\
    Fritz Henglein \> \texttt{<henglein@diku.dk>} \\
    \end{tabbing}
    \end{adjustwidth}
    \newpage
    \ClearWallPaper
%%  ==================================================================
%%  End title page

\chapter*{Abstract}

\newpage

\tableofcontents*

\chapter{Introduction}
High level, memory managed languages offer many productivity advantages because
developers don't have to worry about how objects are arranged in memory. However,
memory layout can hugely affect performance so the productivity advantages can
come with efficiency disadvantages. Much research has been done not only to
quantify this discrepancy but also to narrow the gap.

In this thesis I will be looking at the memory layout of semi-space garbage
collector in the context of a purely functional programming language.
I will implement two optimizations in the garbage collector and evaluate these
optimizations against the baseline. These two novel techniques, as well as the
detailed measurements of their effects, are the primary contributions of this
report.

\section{Jikes RVM vs. native}
\makecomment{Why I use a native GC rather than the Jikes RVM (Research Virtual Machine)
to test my hypothesis even though a lot of other people use it.
Answer: Haskell has different allocation patterns than Java*}

\section{Research Aims}
I have implemented a new semi-space garbage collector for a subset of the
Haskell programming language. This collector contains two novel optimizations
that modify how objects are arranged in memory. It is my hypothesis that these
two optimizations will improve how the garbage collector interacts with the
memory caches. As part of this work I want to answer the following questions:
\begin{itemize}
  \item Is the implementation unreasonably complicated? Some previously proposed
    systems have shown promise on a small scale but were too complicated to be
    used outside of academia.
  \item Are the optimizations beneficial and do the benefits outweigh the costs?
  \item Can the optimizations be ported to collectors used in industry?
\end{itemize}

\section{Thesis organization}
Chapter 2 goes over the current approaches to garbage collection, how people
have tried to solve the problem in the past, and provides details about
LHC which are required by the later chapters.
Chapter 3 describes the design and implementation of the main contribution of
this thesis, as well as the design and implementation of a further optimization
that follows as a direct consequence of the tail-copy technique.
Chapter 4 evaluates these two new approaches against a baseline collector. This
evaluation looks at deterministic factors which are proxies for performance.
Chapter 5 concludes the work and presents possible future work.

\chapter{Background}
\section{Processors, memory and cache}
\makecomment{Diagrams for this section: memory hierarchy.}

CPUs used to run at similar frequencies as main memory, allowing them to read
from memory at roughly the same speed that it could process the information.
Sometimes programs has to process larger data sets than could fit into memory
and parts of the data set would have to be swapped out of main memory and onto
a disk or tape. A lot of caching research from that time was focused on how to
manage the virtual memory using limited physical memory (which supports direct
access with a constant latency) and a spinning disk drive (which requires seeking).

In recent years CPUs have gotten a lot faster, the bandwidth of memory
hasn't been able to keep pace and the memory latency has even gotten worse. This
means that fetching data from memory can easily be the bottleneck when processing
a data set. To solve this, several levels of memory caches were inserted closer
to the CPU. These caches work in levels with the lowest levels being smallest
and fastest. The caches have latencies that are roughly 100x to 10x quicker
than main memory. The caches also have higher bandwidth than main memory but
the benefit is less extreme here.

The introduction of caches meant that memory fetches no longer completed in a
constant amount of time because of the difference between a cache hit and
a cache miss. Just as seen with spinning disks earlier, certain access patterns
and data layouts can significantly improve performance. The canonical problems
that highlights this is matrix multiplication: The elements of the resulting
matrix can be computed in any order but choosing a cache-friendly access pattern
can be orders of magnitude faster.

% Write about MESI and cache associativity

\section{Manual memory management}
Still widely used in low-level languages and on embedded systems.
The onus is on the user to ensure correct usage.
Modern languages are usually garbage collected (8/10 most popular languages on
github\cite{GithubLanguages} in 2017 were garbage collected).
Manual management has low overhead and can finalize objects immediately when
they're no longer used. This is important for limited resources such as file
handlers, file locks, etc.
Manual management often has more predictable performance without arbitrary pauses
but performs less well in terms of throughput and locality.

\section{Reference counting}
\section{Mark-and-sweep}
\makecomment{Diagrams for this section: mark-and-sweep.}

Unlike manual management and reference counting which never scans through all
the heap allocated objects, tracing garbage collection involves finding all live
heap objects by following a root set of objects out through all their children
and children's children. Once all the live objects have been found, unused memory
and resources can be reclaimed.
The mark-and-sweep does exactly this in two phases: First all objects reachable
from the root set are marked, then the "sweep" phases reclaims unused memory and
releases unused OS resources.
This algorithm correctly reclaims cyclic data structures when they're no longer
referenced but, similarly to manual management and reference counting, leads to
fragmentation and thus poor cache performance.

\section{Copying collectors}
\makecomment{Diagrams for this section: semi-space algorithm, moon's algorithm.}

In 1969, Fenichel et al published an algorithm for copying the reachable objects
into newly allocated memory chunk rather than merely marking them.\cite{Fenichel} Once the copying
is complete, all previously used memory will no longer contain live objects
and can be reclaimed. This makes the "sweep" phases unnecessary and it gets rid
of any fragmentation at the cost of requiring 2x available memory. The published
algorithm used stack space to traverse and copy the heap objects which is
undesirable since the stack space is limited and garbage collection will therefore
fail if the heap grows too large. One year later, Cheney published a non-recursive
algorithm for copying live objects and this algorithm has become the foundation
for essentially all modern copying garbage collectors. Since the algorithm is
non-recursive, it uses no stack space and works on heaps at any size.

Cheney's and Fenichel's algorithms both solve the same problem of copying a graph
with Cheney's algorithm having the decisive advantage of working on graphs of
unlimited size. However, the two algorithms also differ in the fact that Fenichel
copied the graph in depth-first order and Cheney copies using breadth-first. This
different copy order affects cache performance but the effects wasn't too pronounced
with the architecture at the time and it wasn't unless 1984 that Moon et al
investigated the effect copy order has with virtual memory. And it wasn't until
1995 that the effects of copy order was quantified in relationship to the
CPU memory cache hierarchy by Nakashima and Chikayama.

Moon modified Cheney's algorithm to have two scanning pointers instead of one.
The to-space is divided up into blocks and the second scanning pointer always
points at the same block as the free pointer. Since objects are read (scavenged)
from the scanning pointer and children are copied (evacuated) to the free pointer,
this means the algorithm often only touches a single block even when the graph
is large and the free pointer is many blocks away from the first scanning pointer.
Since this is optimizing for block/page access, it can perform better when memory
is limited and page faults are expensive.

Moon's algorithm was later improved by Wilson, Lam, and Moher to reduce the
number of objects that are scavenged twice (scavenging is idempotent). Siegwart
and Hirzel further improved performance by adapting the algorithm for
multi-processor machines. Both groups of authors focused primarily on reducing
cache and TLB misses.

\section{Generational collectors}
\makecomment{Diagrams for this section: generational gc.}

It was found empirically that most allocated objects become unreachable (die)
almost immediately and that very few objects live for a significant fraction
of a programs total run-time. This effect of high infant mortality is exploited
by generational collectors by splitting the heap into two or more separate areas
(aka generations) that can be collected independently. A common configuration is
to have a minor and a major generation: New objects are allocated into the minor
generation and promoted into the major generation if they survive a collection.
Since it's relatively rare for older objects to contain pointers to younger
objects, only a small amount of bookkeeping is necessary to collect the minor
generation without tracing through all the live objects in the major generation.
In other words, infant objects can be allocated and collected without tracing or
copying long lived objects. This approach has proved so successful that, even
though the exact rate of infant mortality and object longevity varies from
program to program, generational GC is the default in most popular garbage
collected languages.

\section{Region inference}


\section{Haskell}
\makecomment{Haskell is a purely functional language.\cite{HaskellReport} Mutable objects are
uncommon which leads to a high allocate rate (ie. in Haskell, you create new
objects instead of mutating old ones. How many Haskellers does it take to
change a broken lightbulb? Zero. The lightbulb is immutable. You just create
a new house with a working lightbulb.)}

\section{GHC}
\makecomment{Diagrams for this section: GHC's memory model, GHC's pipeline.}

\makecomment{GHC's object model is assumes an open-world and is extendible.
Objects as pointers has drawbacks, though, and GHC has abandoned the tag-less
model. However, they can only use 2-3 bits (on 32bit and 64bit machines,
respectively) so their tagging is limited. LHC assumes a closed world and therefore
sacrifices extendibility for more tagging bits. LHC can use nearly a full word
for tagging.}

\section{LHC}
\makecomment{Which parts of LHC's architecture is important to describe? \cite{LHC}
Include: Simplicity of the RTS, Compiles subset of Haskell, Generates high-level
LLVM code, Similar memory model to GHC, Whole-program compiler.}

\makecomment{Explain indirections. They are important for the implementation
details of tail-pointer elimination.}

\section{Related work}
\subsection{Depth-first copying (two approaches)}
\subsection{immix}
\subsection{custom object layout (java)}

\chapter{Tail-copying and tail-compaction}

\section{Concept overview}

\makecomment{Diagrams for this section: Copy order and cache lines (depth-first,
breadth-first, tail-first), tail pointer elimination.}

While it has been shown that tracing garbage collectors that use a depth-first
copying algorithm have superior data locality, this technique is not used in any
of the ten most popular languages on GitHub. The overheads in turns of memory
usage and runtime performance are simply too great and the benefits only
manifests themselves in niche situations (eg. with limited physical memory and
a high cost of page faults). However, I think a compromise is possible that
maintains most of the data locality benefits without hampering performance in
the common case, making such an algorithm suitable as the default garbage
collector.

Between depth-first copying and breadth-first copying, there exist a middle-ground
which I call tail-first copying. In this scheme, exactly one child of each node
(the tail) is copied depth-first and the rest of the children (if there are any)
are copied breadth-first. This has several potential benefits.
\begin{itemize}
  \item
  Firstly, as one
  child is likely (but not guaranteed to be) next to its parent, the mutator could
  experience improved locality if it traverses the spine of a data structure (ie.
  down the length of a list or down the right-most path of a tree).
  \item
  Secondly, and
  more importantly, the tail-first copying algorithm operates over blocks of
  tail-linked nodes and is less likely to interleave unrelated nodes. In contrast,
  depth-first copying of two linked lists results in each node of the lists being
  interleaved. And if the heap was extended to include a third list, that list
  would be interleaved with the first two. This means memory is typically not
  copied sequentially which leads to more costly cache misses.
  \item
  The final benefit
  is that tail-first copying does not require any reserved book-keeping data in
  each node nor does it require any extra stack space. The copying algorithm that
  places children next to their parents is tail-recursive (ie it uses a fixed
  amount of stack space), and, just like the breadth-first algorithm, a single
  pointer can be used to keep track of which objects have been scavenged and which
  have not.
\end{itemize}

Tail-first copying in inherently optimistic as there are many situations that
make it impossible to put a child node right after its parent. Each of these
situations have to be detected and they add complexity as well as logic branches
to the algorithm. There are three key cases to consider:
\begin{enumerate}
  \item Multiple parents. Objects may have any number of parents but can only be
  placed next to one. In this case, the child would be placed next to the first
  parent and all subsequently traversed parents will be placed potentially far
  away in memory. I don't believe this to be a big problem as, while having
  multiple is definitely not uncommon, the majority of objects should only have
  a single parent.
  \item
  Unlucky root set traversal. If object A is the parent of object B and the root
  set contains both objects, there's a chance that object B will be copied
  (evacuated) before object A and it therefore cannot be placed immediately
  after object A. This should be a minor problem as root set traversals happen
  infrequently and the size of the root sets are fixed at compile-time.
  \item
  Generational barriers. In generational garbage collectors, objects are
  separated by age into different areas (called generations). Objects in
  separate generations must live apart and cannot be placed right next to each
  other. Fortunately, as objects in older generations rarely point to objects in
  younger generations and due to techniques such as eager promotion (moving an
  object to an older generation if it has parents in that generation), data
  structures are rarely interspersed over many generations. Instead, chunks of
  linked data tends to be located in the same generation, with possible links
  to chunks of data in other generations. As such, I don't believe generational
  garbage collecting would significantly affect the behavior of tail-copying.
  That said, this report will focus exclusively on adding tail-copying to a
  non-generational garbage collector.
\end{enumerate}

For nodes that cannot be placed immediately next to their parent, tail-first
copying is identical to breadth-first copying.
\newline
\newline
After the live objects in a heap have been traced by the tail-first copying
algorithm, many of the objects will have a peculiar property in common: They'll
contain a pointer to a child located right after it. That is, these objects
of size $S$ and located at position $X$, will contain a pointer to a child at
location $X+S$. This common, repeated pattern can be compressed to a single bit
set in the parent's object header: If the bit is set, the last child pointer is
omitted from the object itself and instead calculated as $X+S$.

Switching between breadth-first, depth-first, and tail-first copy order does not
change the sematic layout of the heap objects and this can therefore be done
without changing any other parts of the evaluator or the runtime system. In
contrast, omitting the tail pointer \emph{does} change the memory model and
all systems that interact with the heap objects will need to take it into
account.

\section{Implementation}

\subsection{Baseline garbage collector}\label{algorithm}

Before tail-first copy order and tail-pointer elimination can be implemented,
a semi-space garbage collector needs to be written for LHC. While LHC didn't
ship with a semi-space garbage collector when this project began, it does have
excellent support for finding the roots of the heap which happens to be one of
the most difficult parts of a garbage collector.

The garbage collector that I've implemented in LHC is nearly identical to the
algorithm put forth in Cheney's "A Nonrecursive List Compacting Algorithm"
without any of the improvements that were later invented (such as cache-sized
nurseries, stepping, or generations). The algorithm has five steps and is run
when new allocations have hit a certain limit:
\begin{enumerate}
  \item Allocate a \emph{to-space} of the same size as the previous heap. This
  ensures that there will be enough space to potentially copy the entire heap.
  \item Marking: Evacuate (copy object without copying children) everything in the root-set.
  \item Scavenging: For each object in \emph{to-space}, evacuate its children. Once all
  the objects have been scavenged, the entire heap will have been traced and all
  live objects are copied.
  \item Extend the \emph{to-space} with a new allocation area. This allocation
  area is the size of the \emph{to-space} multiplied by a factor of 3. That is,
  if \emph{to-space} contains, say, 2 megabytes of objects, the new allocation
  area will allow $2*3=6$ megabytes of new objects to be allocated before another
  round of garbage collection is triggered. For performance reasons, the allocation
  area cannot be smaller than 1 megabyte.
  \item Free the old heap and declare \emph{to-space} to be the new heap.
\end{enumerate}

LHC's garbage collection API made it simple to implement the above algorithm
without having to worry how the rest of the compiler is constructed.
Additionally, the garbage collection API is timed with high-precision clocks
to give accurate summary of how much time is spent in the mutator compared to
the garbage collector.

\subsection{Tail-first copy order}

\makecomment{Diagram: Flow-chart of the new 'evacuate' procedure.}

The copy order of the garbage collector is entirely determined by the 'evacuate'
procedure in the algorithm outlined in \ref{algorithm}. This produce takes the
address of a heap object and performs the following steps:
\begin{enumerate}
  \item If the object is an indirection, follow the indirection. This step is
  repeated until an actual heap object is found.
  \item Check if the object is already in the \emph{to-space}. If so, nothing
  more needs to be done and we can exit the procedure.
  \item Lookup the size of the object and copy it to the next free spot in
  \emph{to-space}.
  \item Update the given address of the heap object to point to its new position
  in the \emph{to-space}.
  \item Overwrite the old heap object with an indirection to its new position.
\end{enumerate}

In some Haskell compilers, like GHC, indirections take up two words of memory
(one for the tag and one for the pointer). This can cause problems when you're
trying to overwrite heap objects that are smaller than two words. Luckily, in
LHC, indirections only take up a single word and no special precautions are
necessary. LHC is able to squeeze an indirection into a single word because
all heap objects are word aligned and pointers to them therefore don't use the
least significant 2 or 3 bits (for 32bit and 64bit systems, respectively).

A key thing to note is that the 'evacuate' procedure only copies a single heap
object and not the children of that object. This was the key changed proposed
by Cheney to Fenichel's algorithm which recursively copied all children as well.
\cite{Fenichel,Cheney:1970} Between these two approaches, there's a middle
ground: Recursively copy a single child (if it exists). Doing so would be more
likely to put related objects next to each other than breadth-first copy, and
it still wouldn't require the stack space that a depth-first copy does.

Introducing a check for children to the procedure, and recursing if it is true,
does not add a significant amount of complexity. However, it is important to
verify that the procedure is indeed tail-recursive. Otherwise it'll use an
unbounded amount of stack space and undoubtedly fail for sufficiently large
heaps.

\subsection{Tail-pointer elimination}

Parent nodes can either be compacted or not compacted. Child nodes can be in
three different states (already evacuated, indirection, untouched).

Six different cases:
\begin{itemize}
  \item Compacted parent with already evacuated child.
  \item Compacted parent with indirection as child.
  \item Compacted parent with untouched child.
  \item Parent with already evacuated child.
  \item Parent with indirection as child.
  \item Parent with untouched child.
\end{itemize}

\chapter{Evaluation}

\section{Methodology}
\makecomment{Keywords: 7 test programs with different allocation patterns, 12 measured metrics,
some allocation patterns are faster, some are slower, metrics measured internally
(gc time, mut time, collections, allocated, copied), rest measured
by 'perf', each program was run 5 times.}

\section{Verification}

\section{Results and interpretation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Mutator and GC tables

\begin{table}
  \begin{ltable}{Mutator time}{muttable}
    \data{
    \hline
    \multicolumn{4}{|c|}{Mutator times:}\\
    \hline
    Name&Baseline&Tail-copy&Tail-compact\\
    Allocate&3.3s (±0.5\%)&+0.1\% (±0.2\%)&+0.1\% (±0.5\%)\\
    BinTree&3.4s (±0.1\%)&-0.2\% (±0.3\%)&-0.3\% (±0.2\%)\\
    Bush&2.4s (±0.2\%)&-0.1\% (±0.5\%)&-0.5\% (±0.3\%)\\
    BushTail&3.6s (±0.4\%)&+0.4\% (±0.3\%)&+0.2\% (±0.4\%)\\
    Imbalanced&2.7s (±0.3\%)&+0.0\% (±0.1\%)&-0.2\% (±0.4\%)\\
    MemBench&3.5s (±0.2\%)&+0.1\% (±0.2\%)&+0.0\% (±0.2\%)\\
    PowerSet&6.9s (±0.2\%)&+0.1\% (±0.3\%)&+0.1\% (±0.2\%)\\
    \hline
    \hline
    \multicolumn{2}{|l|}{Min}&-0.2\%&-0.5\%\\
    \multicolumn{2}{|l|}{Max}&0.4\%&0.2\%\\
    \multicolumn{2}{|l|}{Mean}&0.0\%&-0.1\%\\
    \hline

    }
  \end{ltable}
  \begin{rtable}{GC time}{gctable}
    \data{
    \hline
    \multicolumn{4}{|c|}{GC times:}\\
    \hline
    Name&Baseline&Tail-copy&Tail-compact\\
    Allocate&3.8s (±0.3\%)&+19.3\% (±0.2\%)&+9.3\% (±0.1\%)\\
    BinTree&4.5s (±0.2\%)&+15.7\% (±0.3\%)&+10.0\% (±0.1\%)\\
    Bush&2.7s (±0.3\%)&+12.2\% (±0.4\%)&+9.4\% (±0.4\%)\\
    BushTail&3.5s (±0.7\%)&+11.9\% (±0.3\%)&+7.0\% (±1.8\%)\\
    Imbalanced&3.8s (±0.2\%)&+1.4\% (±0.7\%)&-3.8\% (±0.3\%)\\
    MemBench&7.8s (±1.1\%)&-28.1\% (±0.8\%)&-31.7\% (±0.6\%)\\
    PowerSet&12.3s (±0.8\%)&-30.5\% (±0.3\%)&-34.2\% (±0.4\%)\\
    \hline
    \hline
    \multicolumn{2}{|l|}{Min}&-30.5\%&-34.2\%\\
    \multicolumn{2}{|l|}{Max}&19.3\%&10.0\%\\
    \multicolumn{2}{|l|}{Mean}&0.3\%&-4.9\%\\
    \hline
    }
  \end{rtable}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Heap size and allocations tables

\begin{table}
  \begin{ltable}{Collections}{coltable}
    \data{
    \hline
    \multicolumn{4}{|c|}{Heap size:}\\
    \hline
    Name&Baseline&Tail-copy&Tail-compact\\
    Allocate&1.8 gb (±0.0\%)&+0.0\% (±0.0\%)&+0.0\% (±0.0\%)\\
    BinTree&1.5 gb (±0.0\%)&+0.0\% (±0.0\%)&+0.0\% (±0.0\%)\\
    Bush&1.0 gb (±0.0\%)&+0.0\% (±0.0\%)&+0.0\% (±0.0\%)\\
    BushTail&1.6 gb (±0.0\%)&+0.0\% (±0.0\%)&+0.0\% (±0.0\%)\\
    Imbalanced&1.3 gb (±0.0\%)&+0.0\% (±0.0\%)&+0.0\% (±0.0\%)\\
    MemBench&1.4 gb (±0.0\%)&+0.0\% (±0.0\%)&+0.0\% (±0.0\%)\\
    PowerSet&1.5 gb (±0.0\%)&+0.0\% (±0.0\%)&+0.0\% (±0.0\%)\\
    \hline
    \hline
    \multicolumn{2}{|l|}{Min}&0.0\%&0.0\%\\
    \multicolumn{2}{|l|}{Max}&0.0\%&0.0\%\\
    \multicolumn{2}{|l|}{Mean}&0.0\%&0.0\%\\
    \hline
    }
  \end{ltable}
  \begin{rtable}{Allocations}{alloctable}
    \data{
    \hline
    \multicolumn{4}{|c|}{Allocated:}\\
    \hline
    Name&Baseline&Tail-copy&Tail-compact\\
    Allocate&9.0 gb (±0.0\%)&+0.0\% (±0.0\%)&+0.0\% (±0.0\%)\\
    BinTree&9.8 gb (±0.0\%)&+0.0\% (±0.0\%)&+0.0\% (±0.0\%)\\
    Bush&6.6 gb (±0.0\%)&+0.0\% (±0.0\%)&+0.0\% (±0.0\%)\\
    BushTail&5.2 gb (±0.0\%)&+0.0\% (±0.0\%)&+0.0\% (±0.0\%)\\
    Imbalanced&7.8 gb (±0.0\%)&+0.0\% (±0.0\%)&+0.0\% (±0.0\%)\\
    MemBench&9.7 gb (±0.0\%)&+0.0\% (±0.0\%)&+0.0\% (±0.0\%)\\
    PowerSet&15.1 gb (±0.0\%)&+0.0\% (±0.0\%)&+0.0\% (±0.0\%)\\
    \hline
    \hline
    \multicolumn{2}{|l|}{Min}&0.0\%&0.0\%\\
    \multicolumn{2}{|l|}{Max}&0.0\%&0.0\%\\
    \multicolumn{2}{|l|}{Mean}&0.0\%&0.0\%\\
    \hline
    }
  \end{rtable}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Copied and page-faults

\begin{table}
  \begin{ltable}{Copied}{copiedtable}
    \data{
    \hline
    \multicolumn{4}{|c|}{Copied:}\\
    \hline
    Name&Baseline&Tail-copy&Tail-compact\\
    Allocate&4.9 gb (±0.0\%)&+0.0\% (±0.0\%)&-33.3\% (±0.0\%)\\
    BinTree&3.8 gb (±0.0\%)&+0.0\% (±0.0\%)&-25.0\% (±0.0\%)\\
    Bush&2.5 gb (±0.0\%)&+0.0\% (±0.0\%)&-12.5\% (±0.0\%)\\
    BushTail&3.1 gb (±0.0\%)&+0.0\% (±0.0\%)&-19.9\% (±0.0\%)\\
    Imbalanced&3.0 gb (±0.0\%)&+0.0\% (±0.0\%)&-25.0\% (±0.0\%)\\
    MemBench&4.9 gb (±0.0\%)&+0.0\% (±0.0\%)&-20.0\% (±0.0\%)\\
    PowerSet&7.6 gb (±0.0\%)&+0.0\% (±0.0\%)&-20.0\% (±0.0\%)\\
    \hline
    \hline
    \multicolumn{2}{|l|}{Min}&0.0\%&-33.3\%\\
    \multicolumn{2}{|l|}{Max}&0.0\%&-12.5\%\\
    \multicolumn{2}{|l|}{Mean}&0.0\%&-22.2\%\\
    \hline
    }
  \end{ltable}
  \begin{rtable}{Page faults}{pagetable}
    \data{
    \hline
    \multicolumn{4}{|c|}{Page faults:}\\
    \hline
    Name&Baseline&Tail-copy&Tail-compact\\
    Allocate&4.0 mil (±0.0\%)&+0.0\% (±0.0\%)&-10.7\% (±0.0\%)\\
    BinTree&3.6 mil (±0.0\%)&+0.0\% (±0.0\%)&-6.8\% (±0.0\%)\\
    Bush&2.6 mil (±0.0\%)&-0.0\% (±0.0\%)&-3.2\% (±0.0\%)\\
    BushTail&3.3 mil (±0.0\%)&-0.0\% (±0.0\%)&-5.0\% (±0.0\%)\\
    Imbalanced&2.9 mil (±0.0\%)&+0.0\% (±0.0\%)&-7.0\% (±0.0\%)\\
    MemBench&4.1 mil (±0.0\%)&+0.0\% (±0.0\%)&-6.3\% (±0.0\%)\\
    PowerSet&7.7 mil (±0.0\%)&-0.0\% (±0.0\%)&-5.2\% (±0.0\%)\\
    \hline
    \hline
    \multicolumn{2}{|l|}{Min}&-0.0\%&-10.7\%\\
    \multicolumn{2}{|l|}{Max}&0.0\%&-3.2\%\\
    \multicolumn{2}{|l|}{Mean}&-0.0\%&-6.3\%\\
    \hline
    }
  \end{rtable}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Copied and page-faults

\begin{table}
  \begin{ltable}{Cycles}{cycletable}
    \data{
    \hline
    \multicolumn{4}{|c|}{Cycles (billions):}\\
    \hline
    Name&Baseline&Tail-copy&Tail-compact\\
    Allocate&29.3 (±0.7\%)&+11.2\% (±0.4\%)&+4.7\% (±0.4\%)\\
    BinTree&32.5 (±0.3\%)&+9.9\% (±2.9\%)&+6.0\% (±1.4\%)\\
    Bush&21.1 (±3.2\%)&+6.0\% (±0.1\%)&+4.5\% (±0.3\%)\\
    BushTail&29.4 (±0.1\%)&+5.5\% (±0.1\%)&+3.2\% (±0.3\%)\\
    Imbalanced&26.8 (±0.2\%)&+0.8\% (±0.3\%)&-2.4\% (±0.3\%)\\
    MemBench&46.0 (±0.5\%)&-18.8\% (±0.1\%)&-21.7\% (±0.3\%)\\
    PowerSet&79.1 (±0.3\%)&-19.1\% (±0.2\%)&-21.4\% (±0.7\%)\\
    \hline
    \hline
    \multicolumn{2}{|l|}{Min}&-19.1\%&-21.7\%\\
    \multicolumn{2}{|l|}{Max}&11.2\%&6.0\%\\
    \multicolumn{2}{|l|}{Mean}&-0.6\%&-3.9\%\\
    \hline
    }
  \end{ltable}
  \begin{rtable}{Instructions}{insttable}
    \data{
    \hline
    \multicolumn{4}{|c|}{Instructions (billions):}\\
    \hline
    Name&Baseline&Tail-copy&Tail-compact\\
    Allocate&60.0 (±0.1\%)&-3.3\% (±0.1\%)&-1.5\% (±0.1\%)\\
    BinTree&48.6 (±0.4\%)&-1.3\% (±0.3\%)&+0.0\% (±1.0\%)\\
    Bush&32.3 (±0.5\%)&-0.5\% (±0.4\%)&+0.1\% (±0.3\%)\\
    BushTail&40.2 (±0.5\%)&-2.1\% (±0.4\%)&-1.1\% (±0.3\%)\\
    Imbalanced&39.1 (±0.5\%)&-1.4\% (±0.2\%)&+0.2\% (±0.2\%)\\
    MemBench&52.9 (±0.1\%)&-1.4\% (±0.1\%)&-0.1\% (±0.2\%)\\
    PowerSet&92.7 (±0.1\%)&-1.1\% (±0.1\%)&-0.1\% (±0.1\%)\\
    \hline
    \hline
    \multicolumn{2}{|l|}{Min}&-3.3\%&-1.5\%\\
    \multicolumn{2}{|l|}{Max}&-0.5\%&0.2\%\\
    \multicolumn{2}{|l|}{Mean}&-1.6\%&-0.4\%\\
    \hline
    }
  \end{rtable}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% branches and branch misses

\begin{table}
  \begin{ltable}{Branches}{branchtable}
    \data{
    \hline
    \multicolumn{4}{|c|}{Branches (billion):}\\
    \hline
    Name&Baseline&Tail-copy&Tail-compact\\
    Allocate&11.1 (±0.1\%)&-3.7\% (±0.3\%)&-0.8\% (±0.1\%)\\
    BinTree&8.6 (±0.2\%)&-1.5\% (±0.3\%)&+1.7\% (±0.3\%)\\
    Bush&5.7 (±0.6\%)&+1.1\% (±1.0\%)&+2.1\% (±0.3\%)\\
    BushTail&7.1 (±0.5\%)&-2.0\% (±0.3\%)&+0.4\% (±0.3\%)\\
    Imbalanced&6.8 (±0.5\%)&-1.7\% (±0.4\%)&+1.5\% (±0.4\%)\\
    MemBench&9.4 (±0.1\%)&-1.5\% (±0.2\%)&+1.0\% (±0.2\%)\\
    PowerSet&16.2 (±0.2\%)&-1.4\% (±0.0\%)&+0.8\% (±0.1\%)\\
    \hline
    \hline
    \multicolumn{2}{|l|}{Min}&-3.7\%&-0.8\%\\
    \multicolumn{2}{|l|}{Max}&1.1\%&2.1\%\\
    \multicolumn{2}{|l|}{Mean}&-1.5\%&1.0\%\\
    \hline
    }
  \end{ltable}
  \begin{rtable}{Branch misses}{branchmisstable}
    \data{
    \hline
    \multicolumn{4}{|c|}{Branch Misses (million):}\\
    \hline
    Name&Baseline&Tail-copy&Tail-compact\\
    Allocate&8.5 (±4.1\%)&-5.9\% (±0.7\%)&-16.7\% (±0.7\%)\\
    BinTree&30.3 (±0.5\%)&+88.6\% (±0.3\%)&+85.6\% (±0.5\%)\\
    Bush&17.4 (±1.0\%)&+81.5\% (±0.6\%)&+88.2\% (±0.1\%)\\
    BushTail&16.2 (±0.8\%)&+41.2\% (±0.6\%)&+39.0\% (±0.4\%)\\
    Imbalanced&33.7 (±0.4\%)&+9.1\% (±0.3\%)&+5.8\% (±0.4\%)\\
    MemBench&10.6 (±0.9\%)&-8.6\% (±0.7\%)&-13.4\% (±1.0\%)\\
    PowerSet&19.8 (±0.6\%)&-7.3\% (±0.8\%)&-10.9\% (±0.8\%)\\
    \hline
    \hline
    \multicolumn{2}{|l|}{Min}&-8.6\%&-16.7\%\\
    \multicolumn{2}{|l|}{Max}&88.6\%&88.2\%\\
    \multicolumn{2}{|l|}{Mean}&28.4\%&25.4\%\\
    \hline
    }
  \end{rtable}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% LLC loads and LLC misses

\begin{table}
  \begin{ltable}{LLC loads}{llctable}
    \data{
    \hline
    \multicolumn{4}{|c|}{LLC Loads:}\\
    \hline
    Name&Baseline&Tail-copy&Tail-compact\\
    Allocate&9.4 million (±7.4\%)&+36.2\% (±3.3\%)&+4.7\% (±1.4\%)\\
    BinTree&71.2 million (±2.4\%)&-1.0\% (±1.4\%)&-3.8\% (±2.4\%)\\
    Bush&38.4 million (±0.8\%)&+9.6\% (±3.0\%)&+7.4\% (±1.0\%)\\
    BushTail&62.5 million (±5.0\%)&-1.5\% (±3.5\%)&-3.6\% (±2.7\%)\\
    Imbalanced&50.2 million (±0.7\%)&-9.4\% (±1.4\%)&-12.5\% (±1.1\%)\\
    MemBench&186.1 million (±0.3\%)&-87.5\% (±1.6\%)&-87.8\% (±2.9\%)\\
    PowerSet&299.8 million (±0.7\%)&-87.6\% (±1.1\%)&-88.0\% (±5.8\%)\\
    \hline
    \hline
    \multicolumn{2}{|l|}{Min}&-87.6\%&-88.0\%\\
    \multicolumn{2}{|l|}{Max}&36.2\%&7.4\%\\
    \multicolumn{2}{|l|}{Mean}&-20.2\%&-26.2\%\\
    \hline
    }
  \end{ltable}
  \begin{rtable}{LLC load misses}{llcmisstable}
    \data{
    \hline
    \multicolumn{4}{|c|}{LLC Load Misses:}\\
    \hline
    Name&Baseline&Tail-copy&Tail-compact\\
    Allocate&4.5 million (±4.2\%)&+4.1\% (±0.9\%)&-0.7\% (±3.7\%)\\
    BinTree&61.3 million (±1.0\%)&-2.8\% (±2.6\%)&-3.0\% (±1.0\%)\\
    Bush&33.0 million (±2.0\%)&+9.0\% (±2.5\%)&+8.9\% (±3.0\%)\\
    BushTail&53.5 million (±5.7\%)&-3.3\% (±3.6\%)&-5.0\% (±2.8\%)\\
    Imbalanced&42.0 million (±1.4\%)&-12.2\% (±1.0\%)&-10.9\% (±2.5\%)\\
    MemBench&84.0 million (±0.5\%)&-78.6\% (±0.8\%)&-78.4\% (±0.5\%)\\
    PowerSet&132.4 million (±0.8\%)&-78.6\% (±2.3\%)&-78.3\% (±8.1\%)\\
    \hline
    \hline
    \multicolumn{2}{|l|}{Min}&-78.6\%&-78.4\%\\
    \multicolumn{2}{|l|}{Max}&9.0\%&8.9\%\\
    \multicolumn{2}{|l|}{Mean}&-23.2\%&-23.9\%\\
    \hline
    }
  \end{rtable}
\end{table}

\chapter{Conclusion}

% \chapter{Structure}
% \begin{enumerate}
%   \item Context (20\%): Motivate the problem (introduction, why should the reader care,
%     what have other people done)
%   \item Gap (10\%): Give the problem definition and what is missing from the current solutions
%   \item Innovation (50\%): Give the key-idea/detailed methods/experiemnts and evaluation (in that order)
%   \item (20\%) Talk about shortcomings of your approach, and future work, i.e. the conclusion.
% \end{enumerate}
%
% Context:
% \begin{enumerate}
%   \item Sequential access is faster (due to cache)
%   \item Breadth-first GC leads to non-sequential access
%   \item People have tried to solve it with hierarchical copying
%   \item Sequential data allows for pointers to be omitted
% \end{enumerate}
%
% Gap:
% \begin{enumerate}
%   \item Copying GC
%   \item Cheney's algorithm (zero stack, breadth-first ordering)
%   \item Depth-first ordering
%   \item Hierarchical ordering
% \end{enumerate}
%
% Innovation:
% \begin{enumerate}
%   \item Mixed depth-first/breadth-first ordering
%   \item Histogram of distances from parent to children.
%   \item Tail-pointer elimination
% \end{enumerate}
%
%
% \chapter{Introduction}
% Context (20\%): Motivate the problem (introduction, why should the reader care,
% what have other people done)
%
% Motivate the problem: CPUs have gotten a lot faster than memory and caches have been
% the answer, many modern languages use GC and copying can lead to better locality, locality is
% important, copying GC can have better locality than mark-and-sweep,
% describe cheney's algorithm, cheney not cache friendly because breadth-first,
% some tried to solve it by doing depth-first, other's by hierarchical order, immix
% (mixing copying and mark-sweep)
% Changing data layouts can lead to big performance improvements (50\% fewer misses, 20\%
% increased throughput)
%
% {section} cheney's algorithm is widely used but not very cache friendly.
%
% What does that reader need to know? GC, Semi-space GC, mark-and-sweep, cache
% behavior.
%
% Why should the reader care? Cheney algorithm is not cache friendly. Cache friendly
% access can be much faster.
%
% What have other people done? Depth-first copying, hierarchical copying, immix.
%
% \chapter{Gap}
% Give the problem definition and what is missing from the current solutions

\chapter*{Support}

\makecomment{The following quotes show how the references will be used to support
the introduction, implementation decisions, and conclusion. This chapter will
not appear in the final version of this report.}

\blockquote{Recent process speed improvements have far outpaced memory speed
improvements.}\cite{Novark:2006}

\blockquote{Past work has shown that careful placement of objects in memory can
yield performance improvements as high as 42\% in C and C++ applications.}
\cite{Novark:2006}

\blockquote{In general, software references locality can be improved either by
changing a program's data access pattern or its data organization and layout.}
\cite{Chilimbi:1999}

\blockquote{The speed of microprocessors has increased 60\% per year for almost
two decades. Yet, over the same period, the time to access main memory only
decreased at 10\% per year.}
\cite{Chilimbi:1999}

\blockquote{Careful placement of structure elements provides the mechanism to
improve the cache locality of pointer manipulating programs and, consequently,
their performance.}
\cite{Chilimbi:1999}

\blockquote{As processor speeds continue to improve relative to main-memory
access times, cache performance is becoming an increasingly important component
of program performance.}
\cite{Reinhold:1994}

\blockquote{Processors are getting faster, by as factor of 1.5 to 2 per year,
while the speed of main-memory chips is improving only slowly.}
\cite{Reinhold:1994}

\blockquote{In this paper, we propose two copying garbage collection methods.
These two methods copy data structures in depth-first order, while conventional
method copies in breadth-first order. This modification greatly improves memory
access locality during both garbage collection and computation after.\cite{Nakashima:1995}}

\blockquote{In a computer with virtual memory both mark-and-sweep and
reference-counting garbage collection tend to exhibit poor locality of reference
and may run slowly because of excessive page faulting.}
\cite{Chase:1987}

\blockquote{The goal of hierarchical copying is to reduce cache and TLB misses
by colocating objects on the same cache line or page. [...] Paralle hierarchical
copying reduce mutator misses on all measured levels of the memory subsystem:
L1 data cache, combined L2 cache, and TLB.\cite{Siegwart:2006}}


\blockquote{Approximately depth-first copying improves the locality of
data structures copied by the garbage collector.}
\cite{Moon:1984}

\blockquote{A copying garbage collector is free to choose the order in which
it copies accessible objects. It can explit this freedom to improve locality by
copying related objects onto the same page.}
\cite{Moon:1984}

\blockquote{A copying garbage collector can choose breadth-first copying or
depth-first copying. The Cheney list-copying algorithm, which is traditionally
used because it does not require any temporary storage (such as a stack), is
breadth-first.\cite{Moon:1984}}


\blockquote{Depth-first copying generally yields better locality than
breadth-first copying, because it tends to put components of a structure on the
same page as the parent structure.}
\cite{Moon:1984}

\makecomment{Jikes RVM and MMtk are explained here.}\cite{Blackburn:2004}

\blockquote{To reduce fragmentation in mark-region and other non-moving
collectors, this paper introduces opportunistic defragmentation which mixes
copying and marking in a single pass. Using these building blocks, we introduce
immix, a novel high performance garbage collector that combines mark-region with
opportunistic defragmentation to achieve space efficiency, fast collection, and
continguous allocation for mutator performance.\cite{Blackburn:2008}}

\blockquote{This paper shows that, with enough memory on the computer, it is
more expensive to explicitly free a cell than it is to leave it for the garbage
collector - even if the cost of freeing a cell is only a single machine
instruction.}
\cite{Appel:1987}

\blockquote{The traditional mark-and-sweep algorithm, which puts all free cells
onto a linked list for later re-use, takes time proportional to the numebr of
reachable cells plus the number of garbage cells.}
\cite{Appel:1987}

\blockquote{Garbage-collected systems such as Smalltalk, Lisp, or actor languages typically violate some of the basic locality assumptions of modern memory hierarchies, leading to poor performance of normal caching strategies. The main problem is cyclic reuse of memory at a time scale too long to be captured by any caching policy. \cite{Lam:1992}}


\blockquote{Given our understanding of the memory reuse cycle of a generational
garbage collector, we believe it is attractive to exploit the genertional
property by using a cache large enough to hold the youngest generation.]}
\cite{Lam:1992}

\blockquote{In a garbage collected environment, multithreaded programs can run
into an "allocation wall", in which performance is limited by the rate at which
newly allocated data can be written to main memory, and adding more cores does
not improve performance once the limit is reached. One way to avoid the
allocation wall is to use a generational collector with per-thread nurseries
each smaller than the size of the L2 cache, so that most memory accesses hit the
cache rather than main memory. \cite{Marlow:2011}}

\blockquote{The effectiveness problem is that reference counting fails to reclaim
circular structures. If the pointers in a group of objects create a cycle, the
objects' reference counts are never reduced to zero, even if there is no path
to the objects from the root set. \cite{Wilson:1992}}


\blockquote{The efficiency problem with reference counting is that its cost is
generally proportional to the amount of work done by the running program, with a
fairly large constant of proportionality.}
\cite{Wilson:1992}

\blockquote{There are three major problems with traditinal mark-sweep garbage
collectors. First, it is difficult to handle objects of varying sizes without
fragmentation of the available memory. [...] The second problem with mark-sweep
collection is that the cost of a collection is proportional to the size of the
heap, including both live and garbage objects. [...] The third problem involves
locality of reference. Since objects are never moved, the live objects remain in
place after a collection, interspersed with free space. \cite{Wilson:1992}}

\blockquote{The graph of records and pointers is mostly acyclic: newer records
point to older records, and older records do not point to newer records.}
\cite{Appel:1989}

\blockquote{A young object is more likely to become garbage soon. Most objects
have short lifetimes, and only a few objects have long lifetimes. If something
has lasted for a long time already, then it’s probably part of a semi-permanent
data structure; if the object is relatively new, it’s likely to be a temporary
structure holding an intermediate result. \cite{Appel:1989}}

\blockquote{Matrix multiplication is a classical bench-mark for experimenting
with techniques used to exploit machine architecture and to overcome the
limitations of contemporary memory subsystems.}
\cite{Eiron:1999}

\blockquote{This research aims at advancing the state of the art of algorithm
engineering by balancing instruction level parallelism, two levels of data
tiling, copying to provably avoid any cache conficts, and prefetching in
parallel to algorithmic operations, in order to fully exploit the memory
bandwidth. Measurements show that the resultant matrix multiplication algorithm
outperforms IBM's ESSL by 6.8-31.8\%, is less sensitive to the size of the
input data, and scales better. \cite{Eiron:1999}}

\blockquote{As the gap between CPU and memory performance continues to grow, so
does the importance of e ective utilization of the memory hierarchy.}
\cite{Eiron:1999}

\blockquote{Temporal garbage collection (TGC) is based on two heu-ristics:
• Young objects tend lo become garbage quickly.
• The number of pointers from old objects to young objects is relatively small.}
\cite{Courts:1988}

\blockquote{Our measurements show that poor locality in sequential-fit
allocation algorithms reduces program per-formance, both by increasing paging
and cache miss rates. While increased paging can be debilitating on any
architecture, cache misses rates are also important for modern computer
architectures. \cite{Grunwald:1993}}

\blockquote{The algorithm avoids the need for recursion by using the partial
structure as it is built up to keep track of those lists that have been copied.}
\cite{Cheney:1970}

LLVM\cite{LLVM}

LLVM GC support\cite{LLVMGC}

GHC memory model \cite{GHCCommentary}

\bibliographystyle{unsrt}
\bibliography{sources}

% \chapter{Tikz}
% Tikz trial:
%
% \begin{tikzpicture}
%   \draw[step=.5cm,gray,very thin] (-1.4,-1.4) grid (1.4,1.4);
%   \draw (-1.5,0) -- (1.5,0);
%   \draw (0,-1.5) -- (0,1.5);
%   \draw (0,0) circle [radius=1cm];
%   \fill[green!20!white, draw=green!50!black] (0,0) -- (3mm,0mm)
%     arc [start angle=0, end angle=30, radius=3mm] -- cycle;
% \end{tikzpicture}


\end{document}
%%  ==================================================================
%%  End document
